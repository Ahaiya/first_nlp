{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from utils.utils import clean_data\n",
    "\n",
    "\n",
    "class Preprocessor:\n",
    "    def __init__(self):\n",
    "\n",
    "        self.df = pd.DataFrame()        #加载时 暂存数据\n",
    "        self.train_data, self.trian_label = None, None\n",
    "        self.train_x, self.train_y, self.val_x, self.val_y = None, None, None, None\n",
    "\n",
    "        \n",
    "        self._load_xlsx_files()\n",
    "\n",
    "\n",
    "\n",
    "    def _load_xlsx_files(self):\n",
    "        # 加载数据, 并划分 数据集\n",
    "        nums_resume = 0\n",
    "        for sub_dir in os.listdir('./data/resume_data/'):\n",
    "            cur_dir = os.path.join('./data/resume_data/', sub_dir)\n",
    "            if not os.path.isdir(cur_dir):\n",
    "                continue\n",
    "            for filename in os.listdir(cur_dir):\n",
    "                file_path = os.path.join(cur_dir, filename)\n",
    "                if os.path.isfile(file_path):\n",
    "                    nums_resume += 1\n",
    "                    self.read_xlsx(file_path, nums_resume)\n",
    "        self.df = self.df.reset_index(drop=True)\n",
    "\n",
    "        orig_data = self.df\n",
    "        self.train_data, self.trian_label = self._parse_orig_data(orig_data)\n",
    "\n",
    "        \n",
    "        # 划分 数据集                                                                                        \n",
    "        self.train_x, self.val_x, self.train_y, self.val_y = train_test_split(\n",
    "                                                                self.train_data, \n",
    "                                                                self.trian_label, \n",
    "                                                                test_size=0.2, \n",
    "                                                                random_state=0)\n",
    "\n",
    "\n",
    "    def read_xlsx(self, file, id):\n",
    "        data = pd.read_excel(file, header=None, names=['resume', 'label_detail'])\n",
    "        data['resume_id'] = np.full(data.shape[0], id)\n",
    "        data['resume'] = data['resume'].astype(str)\n",
    "        data['label_detail'] = data['label_detail'].astype(str)\n",
    "        data['label'] = data['label_detail'].str.split(\"-\").str.get(0)\n",
    "        self.df = pd.concat([self.df, data])\n",
    "\n",
    "    def _parse_orig_data(self, orig_data):\n",
    "\n",
    "        train_data = orig_data['resume']\n",
    "        label = orig_data['label'].values\n",
    "        train_data = clean_data(train_data).values         # 数据清洗\n",
    "        return train_data, label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = Preprocessor()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = raw.train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "127188"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/km/12m35ksj2q5fvp5_wrw_r5lr0000gn/T/jieba.cache\n",
      "Loading model cost 0.876 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "import os\n",
    "\n",
    "filename = os.listdir('./data/word_dictionaries/')\n",
    "for file in filename:\n",
    "    path = os.path.join('./data/word_dictionaries/', file)\n",
    "    jieba.load_userdict(path)\n",
    "\n",
    "\n",
    "res = []\n",
    "for line in train_data:\n",
    "    seg_list = jieba.cut(line, use_paddle=True)\n",
    "    res.append(' '.join(seg_list))\n",
    "res = pd.Series(res).astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.to_csv('./seg_word/seg_data.csv', sep=',', header=False, index=False, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = pd.read_csv('./seg_word/seg_data.csv', sep=',',header=None).astype(np.str_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume = np.array(res, dtype=np.str_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['简历'],\n",
       "       ['d3e2aa8d52990af80HR92ti0F1dY3429UqaQuejnw'],\n",
       "       ['余刚'],\n",
       "       ...,\n",
       "       ['一种 使用 信号 突然 消失 和 出现 判断 一船 多码 的 系统'],\n",
       "       ['教育 经历'],\n",
       "       ['bookmarkGoBack201009201406 唐山 师范学院 电子信息科学与技术 本科 统招']],\n",
       "      dtype='<U4352')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(f, dtype=np.str_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                        简历\n",
       "1                 d3e2aa8d52990af80HR92ti0F1dY3429UqaQuejnw\n",
       "2                                                        余刚\n",
       "3             188106641721250725373 @ qqcom   男生 日   199109\n",
       "4         武汉   离职 随时 到 岗     6 年 工作 经验   求职意向   Java    ...\n",
       "                                ...                        \n",
       "127183                            船舶 航线 识别方法 装置 电子设备 及 存储介质\n",
       "127184                          船期 相似性 判断 方法 装置 电子设备 及 存储介质\n",
       "127185                    一种 使用 信号 突然 消失 和 出现 判断 一船 多码 的 系统\n",
       "127186                                                教育 经历\n",
       "127187    bookmark   GoBack201009     201406   唐山 师范学院  ...\n",
       "Length: 127188, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resume = f[0].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['锁', 'postman', '正则表达式', 'elasticjobxxljobfiddlerjmeter', '多线程', 'sftp', '等']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resume[10].split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = []\n",
    "for line in resume:\n",
    "    words = line.split(' ')\n",
    "    vocab.extend(words)\n",
    "vocab = list(set(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/soul/opt/anaconda3/envs/deepl/lib/python3.9/site-packages/text2vec/utils/get_file.py:16: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n",
      "2022-08-10 22:11:33.036 | INFO     | text2vec.word2vec:__init__:79 - Load pretrained model:w2v-light-tencent-chinese, path:/Users/soul/.text2vec/datasets/light_Tencent_AILab_ChineseEmbedding.bin\n",
      "2022-08-10 22:11:34.687 | DEBUG    | text2vec.word2vec:__init__:92 - Load w2v from /Users/soul/.text2vec/datasets/light_Tencent_AILab_ChineseEmbedding.bin, spend 1.65 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> (57943, 200)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('..')\n",
    "from text2vec import SentenceModel, EncoderType\n",
    "from text2vec import Word2Vec\n",
    "\n",
    "def compute_emb(model, all_embedding):\n",
    "    # Embed a list of sentences\n",
    "    sentences = vocab\n",
    "    sentence_embeddings = model.encode(sentences)\n",
    "    print(type(sentence_embeddings), sentence_embeddings.shape)\n",
    "\n",
    "    # The result is a list of sentence embeddings as numpy arrays\n",
    "    for sentence, embedding in zip(sentences, sentence_embeddings):\n",
    "        all_embedding[sentence] = embedding\n",
    "        # print(\"Sentence:\", sentence)\n",
    "        # print(\"Embedding shape:\", embedding.shape)\n",
    "        # print(\"Embedding head:\", embedding[:10])\n",
    "        # print()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 中文句向量模型(CoSENT)，中文语义匹配任务推荐，支持fine-tune继续训练\n",
    "    # t2v_model = SentenceModel(\"shibing624/text2vec-base-chinese\",\n",
    "    #                           encoder_type=EncoderType.FIRST_LAST_AVG)\n",
    "    # compute_emb(t2v_model)\n",
    "\n",
    "\n",
    "    # # 支持多语言的句向量模型（Sentence-BERT），英文语义匹配任务推荐，支持fine-tune继续训练\n",
    "    # sbert_model = SentenceModel(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\",\n",
    "    #                             encoder_type=EncoderType.MEAN)\n",
    "    # compute_emb(sbert_model)\n",
    "\n",
    "    # 中文词向量模型(word2vec)，中文字面匹配任务和冷启动适用\n",
    "    all_embedding = {}\n",
    "    w2v_model = Word2Vec(\"w2v-light-tencent-chinese\")\n",
    "    compute_emb(w2v_model, all_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=all_embedding, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./seg_word/word2vec.csv', sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "specialchars = ['<pad>', '<unk>']           #特殊标记？？？\n",
    "vocab = specialchars + list(all_embedding.keys())  #所有词\n",
    "vocab_size = len(vocab)\n",
    "embedding_matrix = np.zeros((vocab_size, 200))\n",
    "\n",
    "#特殊标记， 以均匀分布 产生 词向量\n",
    "for token in specialchars:\n",
    "    all_embedding[token] = np.random.uniform(low=-1, high=1, size=(200))\n",
    "\n",
    "word2idx = {}\n",
    "idx2word = {}\n",
    "# 建立 wordidx 和 idx2word  对应关系\n",
    "for index, word in enumerate(vocab):\n",
    "    word2idx[word] = index\n",
    "    idx2word[index] = word\n",
    "    embedding_matrix[index] = all_embedding[word]      #导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.models.word2vec\n",
    "Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def delelem(data):\n",
    "    \"\"\"\n",
    "    删除多余的标点符号   出现的无用空格   不出现中文字符的数据行\n",
    "    \"\"\"\n",
    "    res_del = []\n",
    "\n",
    "    for line in data:\n",
    "        # line = str(line)\n",
    "        # 使用空字符替换掉间隔符\n",
    "        a = re.sub(r'\\s', '', line)\n",
    "\n",
    "        # 使用精准匹配，匹配连续出现的符号;并用空字符替换他\n",
    "        b = re.sub(r'\\W{2,}', '', a)\n",
    "\n",
    "        # 使用空字符替换空格\n",
    "        c = re.sub(r' ', '', b)\n",
    "\n",
    "        # 删除没有中文的数据行\n",
    "        if len(re.findall(r\"[\\u4e00-\\u9fa5]\", c)) >= 2:\n",
    "            res_del.append(c)\n",
    "\n",
    "    return res_del"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mulunusechar(data):\n",
    "    \"\"\"\n",
    "    生成 特殊符号\n",
    "    \"\"\"\n",
    "    unuse_lis = []\n",
    "\n",
    "    rule_1 = r'\\W'  # 匹配非英文和非数字和非中文\n",
    "    compiled_rule_1 = re.compile(rule_1)      \n",
    "\n",
    "    for line in data:\n",
    "            no_en_and_da = compiled_rule_1.findall(line)\n",
    "            no_en_and_da_str = ''.join(no_en_and_da)\n",
    "            # 只保留 逗号 和 句号\n",
    "            reslis = re.findall(r'^\\S', ''.join(re.findall(r'[^\\，]', ''.join(re.findall(r'[^\\。]', no_en_and_da_str)))))\n",
    "            unuse_lis.append(reslis)        \n",
    "    res = []\n",
    "    for i in unuse_lis:\n",
    "        for j in i:\n",
    "            res.append(j)\n",
    "    res = set(res)\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sympop(data,sym):\n",
    "    \"\"\"\n",
    "    删除指定的特殊符号\n",
    "    :param data: list\n",
    "    :param sym: list or path_of_sym_in_txt\n",
    "    :return: list\n",
    "    \"\"\"\n",
    "    #获得特殊符号的列表\n",
    "    if type(sym)==list:\n",
    "        symlist=sym\n",
    "    else:\n",
    "        symlist = list(sym)\n",
    "\n",
    "    res=[]\n",
    "    for line in data:\n",
    "        for j in range(len(symlist)):     \n",
    "            line = line.replace(symlist[j],'')    \n",
    "        res.append(line)\n",
    "\n",
    "    return res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('deepl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c78cda7ba40eb5ef884d241ef3f0f95599b9e4a1125930213afaa838fee2b78c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
